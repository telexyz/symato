# Đơn giản hóa và hiểu mô hình không gian trạng thái (SSM) với RNN tuyến tính đường chéo
https://arxiv.org/abs/2212.00768

Các mô hình trình tự dựa trên không gian trạng thái tuyến tính (SSM) gần đây đã nổi lên như một lựa chọn kiến trúc đầy hứa hẹn để mô hình hóa các phụ thuộc tầm xa. Tuy nhiên, chúng luôn dựa vào sự rời rạc hóa của một không gian trạng thái liên tục, điều này làm phức tạp thêm sự trình bày và hiểu biết của chúng. Chúng tôi loại bỏ bước rời rạc hóa và đề xuất một mô hình dựa trên RNN tuyến tính đường chéo (DLR: vanilla Diagonal Linear RNNs). Bằng thực nghiệm, chúng tôi cho thấy rằng DLR có hiệu suất ngang với các SSM được đề xuất trước đây với sự giám sát chặt chẽ, mặc dù về mặt khái niệm thì đơn giản hơn nhiều.

Ngoài ra, chúng tôi mô tả tính biểu diễn của SSM (bao gồm cả DLR) và các mô hình dựa trên sự chú ý (attention-based models) thông qua bộ 13 synthetic sequence-to-sequence tasks liên quan đến các tương tác trên hàng chục nghìn tokens, từ các hoạt động đơn giản, chẳng hạn như chuyển dịch một trình tự đầu vào, để phát hiện các đặc điểm hình ảnh đồng phụ thuộc trên các phạm vi không gian dài trong các hình ảnh được làm phẳng.

Chúng tôi thấy rằng mặc dù các SSM cho thấy hiệu suất gần như hoàn hảo đối với các tác vụ có thể được mô hình hóa thông qua một số hạt nhân tích chập (convolutional kernels), nhưng chúng gặp khó khăn với các tác vụ yêu cầu nhiều hạt nhân như vậy và đặc biệt là khi thao tác trình tự mong muốn phụ thuộc vào ngữ cảnh. Ví dụ: DLR học cách dịch chuyển đầu vào dài 0,5M một cách hoàn hảo theo một số vị trí tùy ý nhưng không thành công khi kích thước dịch chuyển phụ thuộc vào ngữ cảnh. Bất chấp những hạn chế này, DLR đạt được hiệu suất cao trên hai tác vụ suy luận bậc cao ListOpsSubTrees và PathfinderSegmentation-256 với độ dài đầu vào lần lượt là 8K và 65K, đồng thời mang lại hiệu suất đáng khích lệ trên PathfinderSegmentation-512 với độ dài đầu vào 262K mà sự chú ý không phải là lựa chọn khả thi.

We find that while SSMs report near-perfect performance on tasks that can be modeled via few convolutional kernels, they struggle on tasks requiring many such kernels and especially when the desired sequence manipulation is context-dependent. For example, DLR learns to perfectly shift a 0.5M-long input by an arbitrary number of positions but fails when the shift size depends on context. Despite these limitations, DLR reaches high performance on two higher-order reasoning tasks ListOpsSubTrees and PathfinderSegmentation-256 with input lengths 8K and 65K respectively, and gives encouraging performance on PathfinderSegmentation-512 with input length 262K for which attention is not a viable choice.